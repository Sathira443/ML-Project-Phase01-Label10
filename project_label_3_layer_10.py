# -*- coding: utf-8 -*-
"""Project_Label_3_Layer_10.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1agp4qt67UmQoCNySIs5mU-h65dj9tKjq

Data Pre-processing
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np

train_data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Project_Phase_1/Label10/train.csv')
valid_data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Project_Phase_1/Label10/valid.csv')
test_data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Project_Phase_1/Label10/test.csv')

"""Define label columns and Prepare the training and validation data"""

label_columns = ['label_1', 'label_2', 'label_3', 'label_4']

df_train_X = train_data.drop(label_columns, axis=1)
df_train_y = train_data['label_3']
df_valid_X = valid_data.drop(label_columns, axis=1)
df_valid_y = valid_data['label_3']

df_train_y.value_counts().plot(kind='bar')

from imblearn.over_sampling import RandomOverSampler

oversampler = RandomOverSampler(sampling_strategy='auto')
df_train_X, df_train_y = oversampler.fit_resample(df_train_X, df_train_y)

df_train_y.value_counts().plot(kind='bar')

"""Standardize the data"""

from sklearn.svm import SVC  # Import Support Vector Machine

# Create an SVM model
model_initial = SVC(kernel='linear')  # You can choose different kernels like 'linear', 'rbf', etc.
model_initial.fit(df_train_X, df_train_y)

y_pred_initial = model_initial.predict(df_valid_X)

from sklearn.metrics import accuracy_score

accuracy_initial = accuracy_score(df_valid_y, y_pred_initial)
print("Initial Accuracy:", accuracy_initial)

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
df_train_X_scaled = scaler.fit_transform(df_train_X)
df_valid_X_scaled = scaler.transform(df_valid_X)

print(df_train_X_scaled.shape)

"""Train an initial K-Nearest Neighbors model"""

from sklearn.svm import SVC  # Import Support Vector Machine

# Create an SVM model
model_initial = SVC(kernel='linear')  # You can choose different kernels like 'linear', 'rbf', etc.
model_initial.fit(df_train_X_scaled, df_train_y)

y_pred_initial = model_initial.predict(df_valid_X_scaled)

"""Predict and find the accuracy"""

from sklearn.metrics import accuracy_score

accuracy_initial = accuracy_score(df_valid_y, y_pred_initial)
print("Initial Accuracy:", accuracy_initial)

"""Use SelectKBest to select the top k features"""

from sklearn.feature_selection import SelectKBest, f_classif

k_best = SelectKBest(score_func=f_classif, k=550)
df_train_X_selected = k_best.fit_transform(df_train_X_scaled, df_train_y)
df_valid_X_selected = k_best.transform(df_valid_X_scaled)

"""Apply PCA to reduce dimensionality"""

from sklearn.decomposition import PCA

pca = PCA(0.95)
pca.fit(df_train_X_selected)
pca_train_X = pca.transform(df_train_X_selected)
pca_valid_X = pca.transform(df_valid_X_selected)

print(pca_train_X.shape)

"""Train a K-Nearest Neighbors model after feature selection and PCA and Predict on the validation set"""

model_after = SVC(kernel='linear')  # You can choose different kernels like 'linear', 'rbf', etc.
model_after.fit(pca_train_X, df_train_y)

y_pred_after = model_after.predict(pca_valid_X)

"""Calculate accuracy after feature selection and PCA"""

from sklearn.metrics import accuracy_score

accuracy_after = accuracy_score(df_valid_y, y_pred_after)
print("Accuracy after Feature Selection and PCA:", accuracy_after)

"""Randomized search for hyperparameter tuning"""

from sklearn.model_selection import GridSearchCV

param_grid = {
    'C': [0.1, 1, 10],  # Adjust the list of values to test
    'kernel': ['linear', 'rbf'],  # You can include other hyperparameters
    # Add more hyperparameters and their respective values as needed
}

grid_search = GridSearchCV(
    verbose=2,
    estimator=SVC(),
    param_grid=param_grid,
    scoring='accuracy',
    cv=5,
    n_jobs=-1
)

# Fit the grid search to your data
grid_search.fit(pca_train_X, df_train_y)

"""Get the best parameters and estimator from the randomized search"""

best_params = grid_search.best_params_
best_estimator = grid_search.best_estimator_

"""Predict on the validation set using the tuned model and find the tuned accuracy"""

y_pred_tuned = best_estimator.predict(pca_valid_X)

# Calculate tuned accuracy
accuracy_tuned = accuracy_score(df_valid_y, y_pred_tuned)
print("Tuned Accuracy:", accuracy_tuned)

best_params

# Drop the "ID" column from the test data
test_data = test_data.drop("ID", axis=1)

# Scale the test data using the same scaler that was fitted on the training data
df_test_X_scaled = scaler.transform(test_data)

# Select the same features using SelectKBest
df_test_X_selected = k_best.transform(df_test_X_scaled)

# Transform the test data using the same PCA model
pca_test_X = pca.transform(df_test_X_selected)

# Use the best-tuned model for predictions on the test data
y_pred_test = best_estimator.predict(pca_test_X)

# Assuming 'y_pred_test' is a numpy array or a list
predictions_df = pd.DataFrame({'label_3': y_pred_test})

# Save predictions to a CSV file
predictions_df.to_csv('svm_predictions_label_3-Layer10.csv', index=False)